{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cef0327-3c5a-4faa-bbf4-19234e40ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision.models as models\n",
    "import torch.cuda.amp as amp\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import numpy as np\n",
    "import gc\n",
    "import IPython\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "checkpoint_dir = \"model_checkpoints\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b27e9c-4fd7-4875-8fb7-e57fa1d08a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed = 42\n",
    "#torch.manual_seed(seed)\n",
    "#np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae546394-da55-4c64-8ae5-77ca1f814fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b9f680-d62f-43fc-8e71-999498499d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Clearing - Free memory: 11034.06 MB / 11172.19 MB\n"
     ]
    }
   ],
   "source": [
    "def check_and_clear_cuda_memory():\n",
    "    free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "    print(f\"Before Clearing - Free memory: {free_mem / 1024**2:.2f} MB / {total_mem / 1024**2:.2f} MB\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    IPython.display.clear_output(wait=True)  # Clear Jupyter output\n",
    "\n",
    "    free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "    print(f\"After Clearing - Free memory: {free_mem / 1024**2:.2f} MB / {total_mem / 1024**2:.2f} MB\")\n",
    "\n",
    "check_and_clear_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f1888f-2616-4bcf-a493-487d30439209",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"tiny-imagenet-200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd763021-e61d-4200-bd03-cabdff460990",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3))\n",
    "])\n",
    "\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d893c3-4cec-4301-9075-3c582afdfcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img_path = \"tiny-imagenet-200/train/n01443537/images/n01443537_0.JPEG\"\n",
    "img = Image.open(img_path)\n",
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b668f16-630b-411e-9bcf-0809603b69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_size = 8000  # Adjust size , max is 100000\n",
    "#train_dataset_full = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_transforms)\n",
    "#indices = np.random.choice(len(train_dataset_full), subset_size, replace=False)  # Fixed subset indices\n",
    "#train_dataset = torch.utils.data.Subset(train_dataset_full, indices)\n",
    "#train_dataset = torch.utils.data.Subset(train_dataset_full, range(subset_size))\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f757ada0-81af-4963-b5b4-b694b495e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8,  pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8,  pin_memory=True,  drop_last=True)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b463564-2b49-4999-b57c-bd537439efc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv7): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (adaptive_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn7 = nn.BatchNorm2d(512)\n",
    "        self.conv8 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn8 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc2 = nn.Linear(512, 200)\n",
    "\n",
    "        self.alpha_conv = nn.Parameter(torch.full((1,), 0.01))  # Learnable Î± for conv layers\n",
    "        self.alpha_fc = nn.Parameter(torch.full((1,), 0.01))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.prelu(self.bn1(self.conv1(x)), self.alpha_conv)\n",
    "        x = F.prelu(self.bn2(self.conv2(x)), self.alpha_conv)\n",
    "\n",
    "        x = F.prelu(self.bn3(self.conv3(x)), self.alpha_conv)\n",
    "        x = F.prelu(self.bn4(self.conv4(x)), self.alpha_conv)\n",
    "\n",
    "        x = F.prelu(self.bn5(self.conv5(x)), self.alpha_conv)\n",
    "        x = F.prelu(self.bn6(self.conv6(x)), self.alpha_conv)\n",
    "\n",
    "        x = F.prelu(self.bn7(self.conv7(x)), self.alpha_conv)\n",
    "        x = F.prelu(self.bn8(self.conv8(x)), self.alpha_conv)\n",
    "\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        x = self.dropout(F.prelu(self.fc1(x), self.alpha_fc))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model with Kaiming initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e3f297-a679-4cd7-86b9-940267acd407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, dataset_name=\"Validation\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1) #Accuracy computation\n",
    "            total += labels.size(0) \n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'{dataset_name} Accuracy: {accuracy:.4f}')\n",
    "    return accuracy\n",
    "\n",
    "train_acc_list, val_acc_list, train_loss_list, val_loss_list, epoch_listings = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "948ba7a4-57d1-48f0-a0f4-bfac2a36e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler):\n",
    "    num_epochs = 500\n",
    "    patience = 5  \n",
    "    min_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss_train = 0.0\n",
    "        correct_train, total_train = 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_train += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss_val = 0.0\n",
    "        correct_val, total_val = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss_val += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        epoch_loss_val = running_loss_val / len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / total_val\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss_train:.4f}, Val Loss: {epoch_loss_val:.4f}')\n",
    "        print(f'Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\\n')\n",
    "\n",
    "        # Early Stopping Check\n",
    "        if epoch_loss_val < min_val_loss:\n",
    "            min_val_loss = epoch_loss_val\n",
    "            patience_counter = 0  # Reset patience\n",
    "            # Save best model\n",
    "            #torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "            print(\"Best model\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            return min_val_loss\n",
    "        \n",
    "    print(\"Training complete.\")\n",
    "    return min_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc575c82-7363-4714-8b8d-55e278c5c18a",
   "metadata": {},
   "source": [
    "Val Loss with (Smoothing: 0.05, LR: 0.05, Weight Decay: 1e-04, Batch Size: 32, Step Size: 1, Gamma: 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2d7e7ce-a35e-4220-a703-3de619c6a8a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Smoothing: 0.05, LR: 0.0001, Weight Decay: 0.0001, Batch Size: 32, Step Size: 4, Gamma: 0.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39mstep_size, gamma\u001b[38;5;241m=\u001b[39mgamma)\n\u001b[1;32m     24\u001b[0m train_loader, val_loader \u001b[38;5;241m=\u001b[39m create_dataloaders(batch_size)  \u001b[38;5;66;03m# Assume a function for this\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m min_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train using your loop\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Loss with (Smoothing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_smoothing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Weight Decay: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Step Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Gamma: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgamma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_val_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 21\u001b[0m running_loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m correct_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_grid = {\n",
    "    'label_smoothing': [0.05],\n",
    "    'lr': [0.0001],\n",
    "    'weight_decay': [1e-4],\n",
    "    'batch_size': [32],\n",
    "    'step_size': [4],\n",
    "    'gamma': [0.5]\n",
    "}\n",
    "\n",
    "# Generate all possible parameter combinations\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "dct = {}\n",
    "for params in param_combinations:\n",
    "    label_smoothing, lr, weight_decay, batch_size, step_size, gamma = params\n",
    "    print()\n",
    "    print(f\"Training with Smoothing: {label_smoothing}, LR: {lr}, Weight Decay: {weight_decay}, Batch Size: {batch_size}, Step Size: {step_size}, Gamma: {gamma}\")\n",
    "\n",
    "    model = CNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) #0.00005\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    train_loader, val_loader = create_dataloaders(batch_size)  # Assume a function for this\n",
    "    min_val_loss = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)  # Train using your loop\n",
    "    print(f\"Val Loss with (Smoothing: {label_smoothing}, LR: {lr}, Weight Decay: {weight_decay}, Batch Size: {batch_size}, Step Size: {step_size}, Gamma: {gamma}):\")\n",
    "    print(f\"{min_val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "907988b8-74d7-4ce2-9418-08df842ff380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 5.1518, Val Loss: 5.6691\n",
      "Train Accuracy: 0.0185, Val Accuracy: 0.0005\n",
      "\n",
      "Best model\n",
      "Epoch 2/20, Train Loss: 4.9579, Val Loss: 6.0212\n",
      "Train Accuracy: 0.0316, Val Accuracy: 0.0092\n",
      "\n",
      "Epoch 3/20, Train Loss: 4.8001, Val Loss: 6.4098\n",
      "Train Accuracy: 0.0430, Val Accuracy: 0.0025\n",
      "\n",
      "Epoch 4/20, Train Loss: 4.6855, Val Loss: 6.8901\n",
      "Train Accuracy: 0.0523, Val Accuracy: 0.0036\n",
      "\n",
      "Epoch 5/20, Train Loss: 4.5902, Val Loss: 6.9913\n",
      "Train Accuracy: 0.0627, Val Accuracy: 0.0050\n",
      "\n",
      "Epoch 6/20, Train Loss: 4.5514, Val Loss: 7.7940\n",
      "Train Accuracy: 0.0665, Val Accuracy: 0.0049\n",
      "\n",
      "Epoch 7/20, Train Loss: 4.5039, Val Loss: 7.9217\n",
      "Train Accuracy: 0.0698, Val Accuracy: 0.0029\n",
      "\n",
      "Epoch 8/20, Train Loss: 4.4640, Val Loss: 7.4658\n",
      "Train Accuracy: 0.0755, Val Accuracy: 0.0053\n",
      "\n",
      "Epoch 9/20, Train Loss: 4.4193, Val Loss: 7.7955\n",
      "Train Accuracy: 0.0800, Val Accuracy: 0.0064\n",
      "\n",
      "Epoch 10/20, Train Loss: 4.3949, Val Loss: 7.9239\n",
      "Train Accuracy: 0.0831, Val Accuracy: 0.0071\n",
      "\n",
      "Epoch 11/20, Train Loss: 4.3723, Val Loss: 8.0317\n",
      "Train Accuracy: 0.0862, Val Accuracy: 0.0064\n",
      "\n",
      "Epoch 12/20, Train Loss: 4.3624, Val Loss: 7.7335\n",
      "Train Accuracy: 0.0886, Val Accuracy: 0.0074\n",
      "\n",
      "Epoch 13/20, Train Loss: 4.3331, Val Loss: 8.0404\n",
      "Train Accuracy: 0.0916, Val Accuracy: 0.0070\n",
      "\n",
      "Epoch 14/20, Train Loss: 4.3248, Val Loss: 8.2638\n",
      "Train Accuracy: 0.0924, Val Accuracy: 0.0087\n",
      "\n",
      "Epoch 15/20, Train Loss: 4.3171, Val Loss: 7.9842\n",
      "Train Accuracy: 0.0932, Val Accuracy: 0.0076\n",
      "\n",
      "Epoch 16/20, Train Loss: 4.3053, Val Loss: 8.1480\n",
      "Train Accuracy: 0.0951, Val Accuracy: 0.0075\n",
      "\n",
      "Epoch 17/20, Train Loss: 4.2951, Val Loss: 8.4542\n",
      "Train Accuracy: 0.0970, Val Accuracy: 0.0088\n",
      "\n",
      "Epoch 18/20, Train Loss: 4.2908, Val Loss: 8.1419\n",
      "Train Accuracy: 0.0965, Val Accuracy: 0.0090\n",
      "\n",
      "Epoch 19/20, Train Loss: 4.2834, Val Loss: 8.2872\n",
      "Train Accuracy: 0.0987, Val Accuracy: 0.0079\n",
      "\n",
      "Epoch 20/20, Train Loss: 4.2825, Val Loss: 8.3322\n",
      "Train Accuracy: 0.0981, Val Accuracy: 0.0086\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "train_acc_list, val_acc_list, train_loss_list, val_loss_list, epoch_listings = [], [], [], [], []\n",
    "\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.5)\n",
    "train_loader, val_loader = create_dataloaders(256)  # Assume a function for this\n",
    "\n",
    "num_epochs = 20\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_listings.append(epoch)\n",
    "    model.train()\n",
    "    running_loss_train = 0.0\n",
    "    correct_train, total_train = 0, 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss_train += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    train_acc_list.append(train_accuracy)\n",
    "    train_loss_list.append(epoch_loss_train)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss_val = 0.0\n",
    "    correct_val, total_val = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss_val += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "    epoch_loss_val = running_loss_val / len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    val_acc_list.append(val_accuracy)\n",
    "    val_loss_list.append(epoch_loss_val)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss_train:.4f}, Val Loss: {epoch_loss_val:.4f}')\n",
    "    print(f'Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\\n')\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if epoch_loss_val < min_val_loss:\n",
    "        min_val_loss = epoch_loss_val\n",
    "        best_model = model\n",
    "        print(\"Best model\")\n",
    "        \n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42a24c9b-e7b1-4e5b-9c56-e882f5aefcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.1190\n",
      "Validation Accuracy: 0.0086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008613782051282052"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Evaluation\n",
    "evaluate_model(best_model, train_loader, \"Training\")\n",
    "\n",
    "# Validation Evaluation\n",
    "evaluate_model(best_model, val_loader, \"Validation\")\n",
    "\n",
    "# Testing Evaluation\n",
    "#evaluate_model(model, test_loader, \"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b44cf16f-3ec8-4fc5-bc51-2afb6ac3dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = os.path.join(checkpoint_dir, 'tiny_imagenet_cnn.pth')\n",
    "torch.save(best_model.state_dict(), final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d15052e1-4cc6-472a-8607-86495d509dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_list = [train_loss_list[i] for i in range(1, 40, 2)]\n",
    "train_loss_list = [train_loss_list[i] for i in range(0, 40, 2)]\n",
    "val_loss_list = [train_loss_list[i] for i in range(1, 40, 2)]\n",
    "train_loss_list = [train_loss_list[i] for i in range(0, 40, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5115704-eb77-4a21-ad73-c0aab684e4d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acc_lis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_acc_lis\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_acc_lis' is not defined"
     ]
    }
   ],
   "source": [
    "train_acc_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75e74f22-8430-4123-a3a4-4d1f2b2a53ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (40,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_listings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining Loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epoch_listings, val_loss_list, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/matplotlib/pyplot.py:3590\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3595\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3596\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1724\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1724\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1725\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (40,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_listings, train_loss_list, label='Training Loss')\n",
    "plt.plot(epoch_listings, val_loss_list, label='Validation Loss', linestyle='dashed')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4aac76-37bc-4d42-b1f9-43dc184ad7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_listings, train_acc_list, label=\"Train Accuracy\")\n",
    "plt.plot(epoch_listings, val_acc_list, label=\"Validation Accuracy\")\n",
    "#plt.plot(epoch_listings, test_acc_list, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Progress\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62175b1e-3fcd-412a-b152-03f99471488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.load_state_dict(torch.load(final_model_path, map_location=device))  # Load model to the correct device\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "def predict_images(image_paths):\n",
    "    predictions = {}\n",
    "    for image_path in image_paths:\n",
    "        image = Image.open(image_path)\n",
    "        image = val_transforms(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = best_model(image)\n",
    "            _, predicted_class = torch.max(output, 1)\n",
    "        predictions[image_path] = train_dataset.dataset.classes[predicted_class.item()]\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20cd50b-7375-497e-b422-28eb14b863d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    \"tiny-imagenet-200/val/images/val_0.JPEG\",\n",
    "    \"tiny-imagenet-200/val/images/val_1.JPEG\",\n",
    "    \"tiny-imagenet-200/val/images/val_2.JPEG\"\n",
    "]\n",
    "predictions = predict_images(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74b159-d2b5-487b-a572-6b38e3a288e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path, predicted_label in predictions.items():\n",
    "    img = Image.open(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.title(predicted_label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a0dc9-9f2f-4587-96b1-0d1873f48994",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e4beb-9d36-47c2-bd03-7b61b2a8003c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
